# -*- coding: utf-8 -*-
"""Copy of Untitled84.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VGI-qmezY4ZwIY21sG7UBAnWN77ly0yY
"""

pip install transformers datasets torch scikit-learn

from google.colab import drive



from google.colab import drive
drive.mount('/content/drive')

folder_path = '/content/drive/MyDrive/R_Arsalan/NLP/healthcare_dataset.csv'
import pandas as pd
from datasets import Dataset

# Load the healthcare dataset
healthcare_data = pd.read_csv(folder_path)

# Ensure that the columns 'Medical Condition' and 'Test Results' exist
if 'Medical Condition' not in healthcare_data.columns or 'Test Results' not in healthcare_data.columns:
    raise ValueError("The dataset does not contain the required columns 'Medical Condition' and 'Test Results'.")

# Select the text and label columns
df = healthcare_data[['Medical Condition', 'Test Results']].copy()

# Rename the columns to 'text' and 'label' for consistency with the existing code
df.columns = ['text', 'label']

# Convert categorical labels to numeric (e.g., 0 for 'Normal', 1 for 'Abnormal', 2 for 'Inconclusive')
df['label'] = df['label'].map({'Normal': 0, 'Abnormal': 1, 'Inconclusive': 2})

# Handle any missing mappings by filling with NaN or a default value
if df['label'].isnull().any():
    df['label'] = df['label'].fillna(-1).astype(int)  # or you can choose another strategy

# Convert to a Hugging Face Dataset
dataset = Dataset.from_pandas(df)

import pandas as pd
from datasets import Dataset
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
import multiprocessing

# Set multiprocessing start method to 'spawn' to avoid deadlocks
multiprocessing.set_start_method('spawn', force=True)

# Load the healthcare dataset
folder_path = '/content/drive/MyDrive/R_Arsalan/NLP/healthcare_dataset.csv'
healthcare_data = pd.read_csv(folder_path)

# Ensure that the columns 'Medical Condition' and 'Test Results' exist
if 'Medical Condition' not in healthcare_data.columns or 'Test Results' not in healthcare_data.columns:
    raise ValueError("The dataset does not contain the required columns 'Medical Condition' and 'Test Results'.")

# Select the text and label columns
df = healthcare_data[['Medical Condition', 'Test Results']].copy()

# Optionally, reduce the dataset size for quicker iterations (e.g., using 10% of the data)
df = df.sample(frac=0.1, random_state=42)

# Rename the columns to 'text' and 'label' for consistency with the existing code
df.columns = ['text', 'label']

# Convert categorical labels to numeric (e.g., 0 for 'Normal', 1 for 'Abnormal', 2 for 'Inconclusive')
df['label'] = df['label'].map({'Normal': 0, 'Abnormal': 1, 'Inconclusive': 2})

# Handle any missing mappings by filling with NaN or a default value
df['label'] = df['label'].fillna(-1).astype(int)

# Convert to a Hugging Face Dataset
dataset = Dataset.from_pandas(df)

# Use a smaller, more efficient model like DistilBERT
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)

# Tokenize the dataset with batching
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_dataset = dataset.map(tokenize_function, batched=True, batch_size=32, cache_file_name="tokenized_dataset.cache")

# Split the dataset into training and evaluation sets
train_test_split = tokenized_dataset.train_test_split(test_size=0.2)
train_dataset = train_test_split['train']
eval_dataset = train_test_split['test']

# Define training arguments with optimizations and improvements
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",  # Updated to the new parameter name
    learning_rate=2e-5,  # You can experiment with this value (e.g., 5e-5 or 1e-5)
    per_device_train_batch_size=4,  # Reduced batch size for lower memory usage
    gradient_accumulation_steps=4,  # Accumulate gradients to simulate a larger batch size
    per_device_eval_batch_size=8,
    num_train_epochs=5,  # Increased the number of epochs for better learning
    weight_decay=0.01,
    fp16=True,  # Enable mixed precision training for faster computations
    dataloader_num_workers=2,  # Reduced to 2 workers to prevent freezing
    load_best_model_at_end=True,  # Load the best model at the end of training
    save_strategy="epoch",  # Save the model at the end of each epoch
    logging_strategy="epoch",  # Log metrics at the end of each epoch
    evaluation_strategy="epoch",  # Evaluate at the end of each epoch
    save_total_limit=2,  # Limit the number of saved models
    metric_for_best_model="eval_loss",  # Use loss as the metric to select the best model
    greater_is_better=False  # Minimize the metric
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=lambda p: {
        "accuracy": (p.predictions.argmax(-1) == p.label_ids).mean()
    }  # Add accuracy as a metric
)

# Fine-tune the model
trainer.train()

# Evaluate the fine-tuned model
eval_results = trainer.evaluate()
print(f"Fine-tuned model evaluation: {eval_results}")

# Load and evaluate the general-purpose DistilBERT model (without fine-tuning)
general_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)
general_trainer = Trainer(
    model=general_model,
    args=training_args,
    eval_dataset=eval_dataset,
    compute_metrics=lambda p: {
        "accuracy": (p.predictions.argmax(-1) == p.label_ids).mean()
    }  # Add accuracy as a metric
)

general_eval_results = general_trainer.evaluate()
print(f"General-purpose model evaluation: {general_eval_results}")